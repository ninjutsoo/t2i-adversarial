# Trustworthiness of Generative AI<span id="head"/>

A collection of papers and resources on the adversarial robustness of Text-to-Image Diffusion Models.

As generative AI rapidly evolves, exemplified by models like Stable Diffusion, understanding their trustworthiness against adversarial attacks becomes crucial. This repository gathers significant contributions that explore word-level adversarial impacts on these models, particularly through the lens of the CLIP mechanism. It serves as a curated knowledge base for advancing the safety and reliability of generative AI systems in the face of nuanced adversarial challenges.

## Overview

This repository offers a curated selection of research on the adversarial robustness of Text-to-Image Diffusion Models, with a focus on word-level adversarial attacks. It aims to enhance the trustworthiness of generative AI by addressing the vulnerabilities exposed by subtle textual manipulations.

## Table of Contents<span id="table-of-contents"/>
* [Trustworthiness of Generative AI](#head)
   * [Table of Contents](#table-of-contents)
   * [T2I Robustness](#t2i-robustness)

## ICL-enhanced LLMs<span id="t2i-robustness"/>
* A Pilot Study of Query-Free Adversarial Attack against Stable Diffusion [pdf](https://arxiv.org/pdf/2303.16378.pdf)
* Black Box Adversarial Prompting for Foundation Models [pdf](https://arxiv.org/pdf/2302.04237.pdf)
* Evaluating the Robustness of Text-to-image Diffusion Models against Real-world Attacks [pdf](https://arxiv.org/pdf/2306.13103.pdf)
